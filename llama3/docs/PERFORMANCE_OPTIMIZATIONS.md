# vLLM Performance Optimizations

## Problem
Message processing time was increasing dramatically:
- 1st message: 1.5s âœ“
- 2nd message: 23.2s âœ—
- 3rd message: 32.4s âœ—âœ—

## Root Causes
1. **No KV cache reuse** - Each message recomputed the entire conversation from scratch
2. **Growing context** - History kept expanding, making each generation slower
3. **Excessive generation** - Default 512 tokens was too high for most responses
4. **No early stopping** - Model continued generating even after completing response

## Implemented Optimizations

### 1. Enable Prefix Caching âš¡ (MOST IMPORTANT)
```python
enable_prefix_caching=True  # Reuse KV cache for repeated prefixes
```
**Impact**: 50-80% speedup by caching system prompt and conversation history

### 2. Aggressive History Trimming ðŸ“‰
- Reduced `max_history_tokens`: 3000 â†’ 2000
- Added `max_history_turns`: Keep only last 6 conversation turns
- Trim by turns first (cheaper than token counting)

**Impact**: Keeps context bounded, prevents exponential slowdown

### 3. Reduced Default Max Tokens ðŸŽ¯
- Changed default: 512 â†’ 256 tokens
- Most responses don't need 512 tokens
- Can still override per-request if needed

**Impact**: 30-50% faster for typical short responses

### 4. Early Stopping Strings ðŸ›‘
```python
stop=["\n\nUser:", "\n\nHuman:", "<|im_end|>", "</s>"]
```
**Impact**: Prevents unnecessary generation after response completes

### 5. Decoding Optimizations ðŸš€
```python
skip_special_tokens=True  # Don't generate special tokens
spaces_between_special_tokens=False  # Faster decoding
use_tqdm=False  # Disable progress bars (reduce overhead)
disable_log_stats=True  # Reduce logging overhead
```

## Expected Performance

### Before:
- 1st msg: ~1.5s
- 2nd msg: ~23s
- 3rd msg: ~32s
- Average: ~12-15s per message (degrading)

### After:
- 1st msg: ~1-2s
- 2nd msg: ~2-4s (with prefix caching)
- 3rd msg: ~2-4s (stable)
- Average: ~2-3s per message (stable)

**Expected improvement: 5-10x faster for multi-turn conversations!**

## How to Use

### Restart with optimizations:
```bash
python vLLMDeploy.py --model ./QwQ-32B --visible_devices "2" --mqtt_username TangClinic --mqtt_password Tang123
```

### Monitor performance:
Watch the logs - you should see:
- Consistent ~2-3s per message (not increasing)
- "prefix caching enabled" in startup logs
- Trimming messages more aggressively

### If you need longer responses:
Override per-message:
```json
{
  "sessionId": "user-123",
  "message": "Explain quantum physics in detail",
  "maxTokens": 512
}
```

## Additional Tips

### For even faster responses:
1. **Use smaller model**: QwQ-32B is huge! Try Llama-3.1-8B for 5-10x speedup
2. **Increase GPU memory**: Set `--gpu_memory_utilization 0.95` for larger cache
3. **Use quantization**: `--quantization awq` for 2x speedup (slight quality loss)

### For better quality (slightly slower):
1. **Increase max_tokens**: `--max_tokens 512`
2. **Keep more history**: `--max_history_tokens 3000`

## Monitoring

Check stats every minute:
```
Stats: Processed=10, Errors=0, AvgLatency=2.5s
```

If AvgLatency keeps increasing:
1. Check if history trimming is working (look for "Dialog trimmed" logs)
2. Verify prefix caching is enabled (startup logs)
3. Consider reducing max_history_turns further

## Technical Details

### Why prefix caching helps:
- System prompt: Computed once, reused forever
- Conversation history: Reused across messages in same session
- Only new user message needs KV computation
- Result: ~80% of tokens are cached in multi-turn chats

### Why history trimming helps:
- QwQ-32B attention is O(nÂ²) with sequence length
- Keeping context small keeps generation fast
- 6 turns (12 messages) is usually enough context
- Users rarely reference messages from >6 turns ago

## Troubleshooting

### Still slow?
1. Check GPU memory: `nvidia-smi` - should be >80% used
2. Verify model is loaded: Look for "vLLM model initialized successfully"
3. Check CUDA version compatibility with vLLM

### Getting errors?
- If "enable_prefix_caching" not supported: Update vLLM `pip install -U vllm`
- If out of memory: Reduce `--gpu_memory_utilization 0.85`

## Conclusion

These optimizations should give you **5-10x speedup** for multi-turn conversations while maintaining response quality. The key is prefix caching + aggressive history management.

Monitor the stats - you should see stable latency around 2-3s per message! ðŸš€
