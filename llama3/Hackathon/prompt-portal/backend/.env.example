# === Backend environment ===
SECRET_KEY=change_me_to_a_random_long_string
ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=60
DATABASE_URL=sqlite:///./app.db

# CORS origins (comma-separated)
CORS_ORIGINS=http://localhost:5173

# === LLM Communication Mode ===
# Choose between 'mqtt' or 'sse'
# - 'mqtt': Use MQTT broker for communication with LLM (traditional mode)
# - 'sse': Use direct HTTP/SSE communication with llama.cpp server (simplified mode)
LLM_COMM_MODE=mqtt

# === MQTT Configuration (used when LLM_COMM_MODE=mqtt) ===
MQTT_BROKER_HOST=47.89.252.2
MQTT_BROKER_PORT=1883
MQTT_CLIENT_ID=prompt_portal_backend
MQTT_USERNAME=TangClinic
MQTT_PASSWORD=Tang123
MQTT_TOPIC_HINT=maze/hint/+
MQTT_TOPIC_STATE=maze/state
MQTT_TOPIC_USER_INPUT=prompt_portal/user_input
MQTT_TOPIC_ASSISTANT_RESPONSE=prompt_portal/assistant_response
MQTT_TOPIC_TEMPLATE=maze/template

# === SSE/Direct HTTP Configuration (used when LLM_COMM_MODE=sse) ===
# LLM server URL (llama.cpp server with OpenAI-compatible API)
LLM_SERVER_URL=http://localhost:8080
# Request timeout in seconds
LLM_TIMEOUT=300
# Default generation parameters
LLM_TEMPERATURE=0.6
LLM_TOP_P=0.9
LLM_MAX_TOKENS=512
# Disable thinking mode for faster responses (set to false for deep reasoning)
LLM_SKIP_THINKING=true
# Maximum tokens to keep in conversation history
LLM_MAX_HISTORY_TOKENS=10000
