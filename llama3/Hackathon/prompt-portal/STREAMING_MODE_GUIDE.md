# Streaming Mode Implementation Guide

## ðŸŽ¯ Overview

The LAM framework now supports **real-time streaming** for chatbot responses, providing a better user experience with word-by-word output instead of waiting for the complete response.

### Key Features
- âœ… **Real streaming in SSE mode** - Direct streaming from llama.cpp server
- âœ… **Simulated streaming in MQTT mode** - Chunks full response for consistent UX
- âœ… **Unified API** - Frontend doesn't need to know which mode is active
- âœ… **Fallback support** - Non-streaming API still available for compatibility

---

## ðŸ—ï¸ Architecture

### SSE Mode (Real Streaming)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    HTTP POST    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   Stream Request  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Frontend â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> â”‚ Backend  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> â”‚  llama.cpp  â”‚
â”‚          â”‚ <â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚ FastAPI  â”‚ <â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚   Server    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  SSE Chunks      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   SSE Chunks     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               (real-time)                    (real-time)
```

**Flow:**
1. Frontend sends message via `/api/chatbot/messages/stream`
2. Backend calls `llm_service.process_message_stream()`
3. llama.cpp generates tokens â†’ Backend forwards â†’ Frontend displays
4. Message saved to database when complete

### MQTT Mode (Simulated Streaming)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    HTTP POST    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      MQTT       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Frontend â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> â”‚ Backend  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> â”‚ MQTT Broker  â”‚
â”‚          â”‚                  â”‚ FastAPI  â”‚                 â”‚   + LAM      â”‚
â”‚          â”‚                  â”‚          â”‚ <â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚              â”‚
â”‚          â”‚ <â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚          â”‚   Full Response â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  SSE Chunks      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             (simulated)      (chunks response)
```

**Flow:**
1. Frontend sends message via `/api/chatbot/messages/stream`
2. Backend waits for full MQTT response
3. Backend chunks the response (10 chars/chunk)
4. Frontend receives and displays chunks
5. Message saved to database

---

## ðŸ“¡ Backend Implementation

### New Endpoint: `/api/chatbot/messages/stream`

**File:** `backend/app/routers/chatbot.py`

**Request Format:**
```json
{
  "session_id": 1,
  "content": "Hello, how are you?",
  "temperature": 0.7,
  "top_p": 0.9,
  "max_tokens": 2048,
  "system_prompt": "You are a helpful assistant",
  "template_id": null
}
```

**Response Format (Server-Sent Events):**

```
data: {"type": "metadata", "session_id": 1, "user_message_id": 123, "session_key": "user1-abc123"}

data: {"type": "chunk", "content": "Hello"}

data: {"type": "chunk", "content": "! I'm"}

data: {"type": "chunk", "content": " doing"}

data: {"type": "chunk", "content": " well."}

data: {"type": "done", "assistant_message_id": 124, "full_content": "Hello! I'm doing well."}
```

**Event Types:**
- `metadata` - Session info, sent immediately
- `chunk` - Text chunk to display
- `done` - Streaming complete, includes full content and message ID
- `error` - Error occurred during streaming

### Code Structure

```python
@router.post("/messages/stream")
async def send_message_stream(payload, db, user):
    # Validate session and template
    # Create user message in DB
    
    async def stream_generator():
        # Send metadata
        yield f"data: {json.dumps(metadata)}\n\n"
        
        if settings.LLM_COMM_MODE.lower() == "sse":
            # Real streaming
            for chunk in llm_service.process_message_stream(...):
                full_content += chunk
                yield f"data: {json.dumps({'type': 'chunk', 'content': chunk})}\n\n"
        else:
            # MQTT mode: simulate streaming
            response = await send_chat_message(mqtt_payload)
            full_content = response.get("content", "")
            
            # Chunk the response
            for i in range(0, len(full_content), 10):
                chunk = full_content[i:i+10]
                yield f"data: {json.dumps({'type': 'chunk', 'content': chunk})}\n\n"
        
        # Save assistant message
        # Send completion event
        yield f"data: {json.dumps({'type': 'done', ...})}\n\n"
    
    return StreamingResponse(stream_generator(), media_type="text/event-stream")
```

---

## ðŸ’» Frontend Implementation

### API Layer: `frontend/src/api.ts`

**New Function:** `chatbotAPI.sendMessageStream()`

```typescript
sendMessageStream: async (
  data: any,
  onMetadata: (metadata: any) => void,
  onChunk: (chunk: string) => void,
  onComplete: (fullContent: string, assistantMessageId: number) => void,
  onError: (error: any) => void
) => {
  const response = await fetch('/api/chatbot/messages/stream', {
    method: 'POST',
    headers: { ... },
    body: JSON.stringify(data)
  })

  const reader = response.body?.getReader()
  const decoder = new TextDecoder()

  // Parse SSE stream
  while (true) {
    const { done, value } = await reader.read()
    if (done) break

    const lines = decoder.decode(value).split('\n')
    for (const line of lines) {
      if (line.startsWith('data: ')) {
        const data = JSON.parse(line.slice(6))
        
        switch (data.type) {
          case 'metadata': onMetadata(data); break
          case 'chunk': onChunk(data.content); break
          case 'done': onComplete(data.full_content, data.assistant_message_id); break
          case 'error': onError(new Error(data.message)); break
        }
      }
    }
  }
}
```

### Chat Component: `frontend/src/pages/ChatStudio.tsx`

**Updated `handleSendMessage` Function:**

```typescript
const handleSendMessage = async (text?: string) => {
  // ... validation ...
  
  let streamedContent = ''
  let tempAssistantId = -Date.now()
  
  await chatbotAPI.sendMessageStream(
    { session_id, content, temperature, ... },
    
    // onMetadata
    (metadata) => {
      // Create placeholder for assistant message
      const placeholderAssistant = {
        id: tempAssistantId,
        role: 'assistant',
        content: '',
        created_at: new Date().toISOString()
      }
      setMessages(prev => [...prev, placeholderAssistant])
      setStreamingMessageId(tempAssistantId)
    },
    
    // onChunk
    (chunk) => {
      streamedContent += chunk
      setMessages(prev => prev.map(msg => 
        msg.id === tempAssistantId 
          ? { ...msg, content: streamedContent }
          : msg
      ))
    },
    
    // onComplete
    async (fullContent, realAssistantId) => {
      // Update with final content and real ID
      setMessages(prev => prev.map(msg => 
        msg.id === tempAssistantId 
          ? { ...msg, id: realAssistantId, content: fullContent }
          : msg
      ))
      setStreamingMessageId(null)
    },
    
    // onError
    (error) => {
      setErrorMessage(`Failed: ${error.message}`)
      setMessages(prev => prev.filter(msg => msg.id !== tempAssistantId))
    }
  )
}
```

---

## ðŸ”„ Comparison: Old vs New

### Old Implementation (Non-Streaming)

**Backend:**
```python
# Wait for complete response
response = llm_service.process_message(...)
assistant_message.content = response  # Full response at once
```

**Frontend:**
```typescript
const res = await chatbotAPI.sendMessage(...)
const assistantMsg = res.data.assistant_message

// Simulate streaming (chunking already-received content)
simulateStreamingMessage(assistantMsg.id, assistantMsg.content)
```

**User Experience:**
1. User sends message
2. Wait... (no feedback)
3. Full response appears, then chunks display (simulated)

### New Implementation (Real Streaming)

**Backend (SSE Mode):**
```python
# Stream as generated
for chunk in llm_service.process_message_stream(...):
    yield chunk  # Send immediately
```

**Backend (MQTT Mode):**
```python
# Wait, then chunk
response = await send_chat_message(...)
for chunk in chunk_response(response):
    yield chunk
```

**Frontend:**
```typescript
await chatbotAPI.sendMessageStream(
  data,
  onMetadata,
  (chunk) => appendToMessage(chunk),  // Real-time display
  onComplete,
  onError
)
```

**User Experience:**
1. User sends message
2. Words appear immediately as generated (SSE) or chunked (MQTT)
3. Natural typing effect

---

## âš™ï¸ Configuration

No configuration changes needed! Streaming works automatically based on `LLM_COMM_MODE`:

```bash
# SSE Mode - Real streaming from llama.cpp
LLM_COMM_MODE=sse
LLM_SERVER_URL=http://localhost:8080

# MQTT Mode - Simulated streaming
LLM_COMM_MODE=mqtt
MQTT_BROKER_HOST=47.89.252.2
```

---

## ðŸ§ª Testing

### Test Streaming Endpoint (Backend)

**SSE Mode:**
```bash
curl -X POST "http://localhost:3000/api/chatbot/messages/stream" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "session_id": 1,
    "content": "Tell me a short story"
  }' \
  --no-buffer
```

**Expected Output:**
```
data: {"type":"metadata","session_id":1,...}

data: {"type":"chunk","content":"Once"}

data: {"type":"chunk","content":" upon"}

data: {"type":"chunk","content":" a"}

data: {"type":"chunk","content":" time"}
...
data: {"type":"done","assistant_message_id":123,"full_content":"Once upon a time..."}
```

### Test Frontend

1. Open ChatStudio page
2. Send a message
3. Observe word-by-word appearance
4. Check browser DevTools Network tab â†’ See `messages/stream` with streaming response

---

## ðŸ› Troubleshooting

### Issue: No streaming, full response appears at once

**Cause:** Nginx buffering enabled

**Solution:** Add to Nginx config:
```nginx
location /api/chatbot/messages/stream {
    proxy_pass http://backend;
    proxy_buffering off;
    proxy_cache off;
    proxy_set_header X-Accel-Buffering no;
}
```

### Issue: Chunks appear too fast in MQTT mode

**Cause:** Chunk delay too small

**Solution:** Adjust in `chatbot.py`:
```python
chunk_size = 10  # Increase for larger chunks
await asyncio.sleep(0.01)  # Increase delay (e.g., 0.05)
```

### Issue: Streaming stops midway

**Cause:** Connection timeout or error

**Solution:**
- Check backend logs for exceptions
- Verify llama.cpp server is running (SSE mode)
- Check MQTT broker connection (MQTT mode)

### Issue: Error "Response body is not readable"

**Cause:** Browser doesn't support streaming

**Solution:** Use modern browser (Chrome, Firefox, Edge)

---

## ðŸ“Š Performance Metrics

### SSE Mode
- **Time to First Token:** ~200-500ms
- **Tokens/Second:** 20-50 (depends on GPU)
- **Latency:** Near real-time

### MQTT Mode
- **Time to First Chunk:** ~1-2s (after full response)
- **Chunks/Second:** 100 (configurable)
- **Latency:** Response time + chunking time

---

## ðŸŽ“ Best Practices

### 1. Handle Errors Gracefully
Always implement `onError` callback:
```typescript
onError: (error) => {
  console.error('Streaming failed:', error)
  setErrorMessage('Connection lost. Please try again.')
  // Cleanup partial message
}
```

### 2. Show Streaming Indicator
```typescript
{streamingMessageId && (
  <div className="streaming-indicator">
    <Loader2 className="animate-spin" />
    Generating response...
  </div>
)}
```

### 3. Allow Cancellation
```typescript
const abortController = new AbortController()

// Pass signal to fetch
fetch(url, { signal: abortController.signal, ... })

// Cancel button
<button onClick={() => abortController.abort()}>Cancel</button>
```

### 4. Optimize Chunk Size
```python
# SSE mode: No control (token-by-token from llama.cpp)

# MQTT mode: Adjust for best UX
chunk_size = 10  # Good for typical responses
chunk_size = 5   # Slower, more dramatic
chunk_size = 20  # Faster, less smooth
```

### 5. Preserve Conversation Flow
Ensure messages are added in order:
```typescript
// Use temporary ID for streaming message
const tempId = -Date.now()

// Replace with real ID when complete
setMessages(prev => prev.map(msg => 
  msg.id === tempId ? { ...msg, id: realId } : msg
))
```

---

## ðŸš€ Future Enhancements

### Planned
- [ ] Maze game streaming hints
- [ ] Progress indicators for long responses
- [ ] Streaming cancellation support
- [ ] Token usage tracking per stream
- [ ] Streaming for template testing page

### Potential
- [ ] Voice output during streaming
- [ ] Multi-modal streaming (text + images)
- [ ] Streaming with intermediate thoughts
- [ ] Real-time collaboration (multiple users)

---

## ðŸ“š Related Documentation

- **[DUAL_MODE_QUICK_REFERENCE.md](./DUAL_MODE_QUICK_REFERENCE.md)** - Mode configuration
- **[SSE_MODE_DEPLOYMENT.md](./SSE_MODE_DEPLOYMENT.md)** - SSE setup guide
- **[COMPLETE_SSE_IMPLEMENTATION.md](./COMPLETE_SSE_IMPLEMENTATION.md)** - Full implementation

---

## âœ… Summary

**Status:** âœ… **COMPLETE - Ready for Production**

The streaming implementation provides:
- Real-time response streaming in SSE mode
- Simulated streaming in MQTT mode for consistency
- Unified API for both modes
- Better user experience with immediate feedback
- Backward compatibility with non-streaming API

**Key Files Modified:**
- `backend/app/routers/chatbot.py` - Added `/messages/stream` endpoint
- `frontend/src/api.ts` - Added `sendMessageStream()` function
- `frontend/src/pages/ChatStudio.tsx` - Updated to use streaming API

**User Impact:**
- **Before:** Wait for full response, then see simulated typing
- **After:** See words appear in real-time as generated

**No Breaking Changes:** Non-streaming API (`/messages`) still available for compatibility.
