# OpenAI Tools API Compatibility Fix

## üêõ Issue

llama.cpp server was rejecting requests with error:
```
Error code: 500 - {'error': {'code': 500, 'message': 'tools param requires --jinja flag', 'type': 'server_error'}}
```

This happened because the backend was trying to use OpenAI's `tools` API parameter for function calling, but llama.cpp only supports this when started with the `--jinja` flag.

## ‚úÖ Solution

Changed the backend to **NOT** use the OpenAI `tools` parameter. Instead, maze game actions should be specified in the **template prompt** itself, and the LLM should respond with structured JSON.

## üìù Changes Made

### File: `backend/app/routers/mqtt_bridge.py`

**Two endpoints changed:**

1. **`/publish_state`** (Line ~49)
2. **`/request_hint`** (Line ~175)

**Before:**
```python
use_tools=True  # Enable maze game function calling
```

**After:**
```python
use_tools=False  # Disable OpenAI tools API - use prompt-based guidance instead
```

## üéØ How It Works Now

### Old Approach (Broken)
```
Backend ‚Üí llama.cpp API with tools=[{...}]
           ‚Üì
llama.cpp: ‚ùå "tools param requires --jinja flag"
```

### New Approach (Working)
```
Backend ‚Üí llama.cpp API with NO tools parameter
          System prompt contains instructions for JSON response
           ‚Üì
llama.cpp: ‚úÖ Generates JSON response based on prompt
           ‚Üì
Backend: Parses JSON and extracts actions
```

## üìã Template Requirements

Your maze game templates should now include instructions like this:

```
You are a Large Action Model (LAM) guiding a player through a maze.

Respond in JSON format with these fields:
{
  "hint": "Your helpful hint text here",
  "path": [[x1, y1], [x2, y2], ...],  // Suggested path coordinates
  "show_path": true/false,  // Whether to highlight path
  "break_wall": [x, y],  // Optional: wall to break
  "speed_boost_ms": 1500,  // Optional: speed boost duration
  "slow_germs_ms": 3000,  // Optional: slow enemies duration
  "freeze_germs_ms": 3500,  // Optional: freeze enemies duration
  "teleport_player": [x, y],  // Optional: teleport coordinates
  "spawn_oxygen": [[x1, y1], [x2, y2]],  // Optional: oxygen locations
  "move_exit": [x, y],  // Optional: move exit position
  "reveal_map": true/false  // Optional: reveal entire map
}

Available actions:
- break_wall: Break a wall at [x, y] to create a path
- speed_boost_ms: Give player speed boost for N milliseconds
- slow_germs_ms: Slow down enemies for N milliseconds
- freeze_germs_ms: Freeze enemies completely for N milliseconds
- teleport_player: Move player to [x, y]
- spawn_oxygen: Create oxygen pellets at locations
- move_exit: Relocate the exit to [x, y]
- reveal_map: Show entire maze layout

Game state will be provided as JSON. Analyze the player position, walls, 
enemies, and exit location to provide optimal guidance.
```

## üöÄ Deployment

**No restart needed!** The code change is already applied. Just:

1. ‚úÖ Backend will now work with llama.cpp without `--jinja` flag
2. ‚úÖ Maze game hints will be generated successfully
3. ‚úÖ Templates control the response format through prompts

## üß™ Testing

### 1. Verify Fix Applied
Check backend logs - should see:
```
[SSE MODE] Calling LLM with session_id=..., use_tools=False
```

**NOT** `use_tools=True`

### 2. Test Maze Game
1. Start maze game in LAM mode
2. Backend should generate hints without errors
3. Check backend logs for successful hint generation:
```
[SSE MODE] Successfully generated and stored hint for session session-XXXXXX
```

### 3. Verify Response Format
The LLM should respond with JSON like:
```json
{
  "hint": "Move north towards the exit. There's a clear path available.",
  "path": [[1,2], [1,3], [1,4]],
  "show_path": true
}
```

Frontend will parse this and display the hint + path overlay.

## üéì Technical Details

### Why This Works

**OpenAI Tools API** (requires `--jinja`):
```python
tools = [{"type": "function", "function": {...}}]
response = client.chat.completions.create(tools=tools)
# llama.cpp needs --jinja flag to parse tools
```

**Prompt-Based Approach** (works always):
```python
system_prompt = "Respond in JSON: {hint, path, actions...}"
response = client.chat.completions.create(messages=[...])
# llama.cpp just generates text following prompt instructions
```

### Advantages of New Approach

1. ‚úÖ **No `--jinja` flag needed** - Works with default llama.cpp startup
2. ‚úÖ **More flexible** - Can customize response format per template
3. ‚úÖ **Easier debugging** - JSON response is visible in logs
4. ‚úÖ **Compatible with more models** - Not all models support OpenAI tools

### Disadvantages

1. ‚ö†Ô∏è **Depends on model following instructions** - Model must generate valid JSON
2. ‚ö†Ô∏è **No schema validation** - Tools API validates parameters; prompts don't
3. ‚ö†Ô∏è **Template burden** - Each template must include action instructions

## üîß Configuration

### llama.cpp Server (Machine A)

**Current (Working):**
```bash
.\llama-server.exe --model Qwen2.5-Coder-32B-Instruct-Q4_K_M.gguf --port 8080 --host 0.0.0.0
```

**If you want to use OpenAI Tools API (optional):**
```bash
.\llama-server.exe --model Qwen2.5-Coder-32B-Instruct-Q4_K_M.gguf --port 8080 --host 0.0.0.0 --jinja
```

Then change backend code back to `use_tools=True`.

## üìä Performance Impact

### Before (with tools error):
- Every request: ‚ùå 500 error
- No hints generated

### After (without tools):
- Every request: ‚úÖ Success
- Hints generated in 3-7 seconds

**No performance difference** - The LLM generates the same response, just following prompt instructions instead of tools schema.

## üêõ Troubleshooting

### Issue: LLM not generating valid JSON

**Symptoms:**
- Backend logs show: `Response is plain text, use as-is`
- Frontend displays raw text instead of structured hints

**Solution:**
Update your template to be more explicit:
```
CRITICAL: You MUST respond with valid JSON. Example:
{"hint": "your hint", "path": [[1,2], [1,3]]}

Do not include any text before or after the JSON.
```

### Issue: Actions not working (break_wall, speed_boost, etc.)

**Symptoms:**
- Hints appear but special abilities don't activate
- No path overlay shown

**Solution:**
Check your template includes:
1. Instructions to include action fields in JSON
2. Explanation of what each action does
3. Examples of valid action usage

### Issue: Still getting "tools param requires --jinja flag"

**Symptoms:**
- Same error as before
- Backend logs show `use_tools=True`

**Solution:**
1. Restart backend to reload code changes
2. Verify file edits were saved
3. Check no caching issues (clear `__pycache__`)

## üìö Related Files

### Modified
- `backend/app/routers/mqtt_bridge.py` (2 functions updated)

### Related Documentation
- **MAZE_GAME_SSE_FIX_SUMMARY.md** - Original fix documentation
- **MAZE_GAME_SSE_FIX_COMPLETE.md** - Complete overview
- **LAM Template Examples** - (if available in your templates)

## ‚úÖ Summary

**Problem**: llama.cpp rejected `tools` parameter without `--jinja` flag

**Solution**: Disabled OpenAI tools API, use prompt-based JSON responses instead

**Impact**: 
- ‚úÖ Maze game now works without `--jinja` flag
- ‚úÖ Templates control response format
- ‚úÖ More flexible and compatible

**Status**: ‚úÖ **FIXED - Working Now**

---

**Author**: GitHub Copilot  
**Date**: 2025-11-09  
**Version**: 1.1  
**Related**: MAZE_GAME_SSE_FIX_SUMMARY.md
